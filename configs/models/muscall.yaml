 model_config: 
  model_name: muscall
  projection_dim: 512             # dimensionality of the multimodal projection layer
  temperature: null
  audio: 
    model: ModifiedResNet         # name of the audio backbone model (ModifiedResNet supported)
    pooling: attention            # pooling mechanism to obtain fixed-size audio features. One of [average, attention]
    audio_len_seconds: 20         # max lenght in seconds
    hidden_size: 256              # 
    conv_out_channels: 16         # number of output channels in the resnet conv layers
    n_mels: 128                   # number of mel filterbanks to use in melspectrogram
    sample_rate: 16000            # sample rate of the input audio
    n_fft: 1024                   # size of the FFT
    f_min: 0                      # min frequency in the spectrogram
    f_max: 11025                  # max frequency in the spectrogram
    ssl:
      do_ssl: False               # whether to add audio self-supervised learning during pre-training
      ssl_loss_weight: 0.3
      ssl_temperature: 0.5
      ssl_projection_dim: 128
      p_polarity: 0.8
      p_noise: 0.3
      p_gain: 0.2
      p_pitch_shift: 0.4
  text: 
    model: TextTransformer         # name of the textual head model. One of TextTransformer, CLIPTextModel
    pretrained: openai/clip-vit-base-patch32 # name of the pretrained textual head model
    frozen_layers: null         # range of layers to freeze during pretraining
    num_hidden_layers: 4          # number of hidden layers in the transformer
    hidden_size: 512              # dimensionality of the transformer layers
    num_attention_heads: 8        # number of attention heads in the transformer
    vocab_size: 49408             #
    max_position_embeddings: 77   # max number of tokens (seq is truncated if longer)
    attention_dropout: 0.2        # dropout probability for the attention weights
    dropout: 0.2                  # dropout probaility in the feedforward blocks
  loss: clip                      # one of [clip, weighted_clip]
